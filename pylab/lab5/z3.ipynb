{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=2, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Testowanie: num_layers=3, neurons=64, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=relu, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=adam, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.01, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=32, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testowanie: num_layers=3, neurons=128, activation=tanh, optimizer=sgd, learning_rate=0.001, batch_size=64, epochs=100\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Najlepszy wynik:\n",
      "num_layers              3\n",
      "neurons               128\n",
      "activation           relu\n",
      "optimizer            adam\n",
      "learning_rate        0.01\n",
      "batch_size             64\n",
      "epochs                 50\n",
      "accuracy         0.983333\n",
      "Name: 98, dtype: object\n",
      "     num_layers  neurons activation optimizer  learning_rate  batch_size  \\\n",
      "98            3      128       relu      adam          0.010          64   \n",
      "100           3      128       relu      adam          0.001          32   \n",
      "3             2       64       relu      adam          0.010          64   \n",
      "115           3      128       tanh      adam          0.010          64   \n",
      "101           3      128       relu      adam          0.001          32   \n",
      "..          ...      ...        ...       ...            ...         ...   \n",
      "79            3       64       relu       sgd          0.001          64   \n",
      "110           3      128       relu       sgd          0.001          64   \n",
      "30            2       64       tanh       sgd          0.001          64   \n",
      "14            2       64       relu       sgd          0.001          64   \n",
      "78            3       64       relu       sgd          0.001          64   \n",
      "\n",
      "     epochs  accuracy  \n",
      "98       50  0.983333  \n",
      "100      50  0.980556  \n",
      "3       100  0.980556  \n",
      "115     100  0.977778  \n",
      "101     100  0.977778  \n",
      "..      ...       ...  \n",
      "79      100  0.700000  \n",
      "110      50  0.675000  \n",
      "30       50  0.650000  \n",
      "14       50  0.486111  \n",
      "78       50  0.422222  \n",
      "\n",
      "[128 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "# Przygotowanie danych\n",
    "def load_data(load_function):\n",
    "    \"\"\"\n",
    "    X_train (numpy.ndarray) - zbiór danych treningowych\n",
    "    X_test (numpy.ndarray) - zbiór danych testowych\n",
    "    y_train (numpy.ndarray) - etykiety danych treningowych\n",
    "    y_test (numpy.ndarray) - etykiety danych testowych\n",
    "    class_num (int) - liczba klas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Wczytanie danych\n",
    "    data = load_function()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    \n",
    "    # Przekształcenie etykiet na format one-hot ( 1 lub 0 )\n",
    "    y = pd.Categorical(y)\n",
    "    y = pd.get_dummies(y)\n",
    "    class_num = y.shape[1]\n",
    "    \n",
    "    # Podział na zbiór treningowy i testowy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Skalowanie danych\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, class_num\n",
    "\n",
    "# Funkcja do budowy modelu\n",
    "def build_model(num_layers, neurons, activation, optimizer, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X.shape[1],)))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(class_num, activation=\"softmax\"))\n",
    "    \n",
    "    # Wybór optymalizatora\n",
    "    if optimizer == \"adam\":\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == \"sgd\":\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Nieobsługiwany optymalizator\")\n",
    "    \n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Parametry do testowania\n",
    "param_grid = {\n",
    "    \"num_layers\": [2, 3],  # Liczba warstw\n",
    "    \"neurons\": [64, 128],  # Liczba neuronów w warstwie\n",
    "    \"activation\": [\"relu\", \"tanh\"],  # Funkcja aktywacji\n",
    "    \"optimizer\": [\"adam\", \"sgd\"],  # Optymalizator\n",
    "    \"learning_rate\": [0.01, 0.001],  # Prędkość nauczania\n",
    "    \"batch_size\": [32, 64],  # Wielkość batcha\n",
    "    \"epochs\": [50, 100]  # Liczba epok\n",
    "}\n",
    "\n",
    "# Wyniki\n",
    "results = []\n",
    "\n",
    "# Przeszukiwanie siatkowe\n",
    "\n",
    "X_train, X_test, y_train, y_test, class_num = load_data(load_digits)\n",
    "\n",
    "for num_layers in param_grid[\"num_layers\"]:\n",
    "    for neurons in param_grid[\"neurons\"]:\n",
    "        for activation in param_grid[\"activation\"]:\n",
    "            for optimizer in param_grid[\"optimizer\"]:\n",
    "                for learning_rate in param_grid[\"learning_rate\"]:\n",
    "                    for batch_size in param_grid[\"batch_size\"]:\n",
    "                        for epochs in param_grid[\"epochs\"]:\n",
    "                            print(f\"Testowanie: num_layers={num_layers}, neurons={neurons}, activation={activation}, \"\n",
    "                                  f\"optimizer={optimizer}, learning_rate={learning_rate}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                            \n",
    "                            # Budowanie modelu\n",
    "                            model = build_model(num_layers, neurons, activation, optimizer, learning_rate)\n",
    "                            \n",
    "                            # Trenowanie modelu\n",
    "                            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.2)\n",
    "                            \n",
    "                            # Ewaluacja\n",
    "                            y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "                            y_true = np.argmax(y_test.values, axis=1)\n",
    "                            accuracy = accuracy_score(y_true, y_pred)\n",
    "                            \n",
    "                            # Zapisanie wyników\n",
    "                            results.append({\n",
    "                                \"num_layers\": num_layers,\n",
    "                                \"neurons\": neurons,\n",
    "                                \"activation\": activation,\n",
    "                                \"optimizer\": optimizer,\n",
    "                                \"learning_rate\": learning_rate,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"accuracy\": accuracy\n",
    "                            })\n",
    "\n",
    "# Konwersja wyników na DataFrame\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Wyświetlenie najlepszej konfiguracji\n",
    "best_result = results_df.loc[results_df[\"accuracy\"].idxmax()]\n",
    "print(\"Najlepszy wynik:\")\n",
    "print(best_result)\n",
    "\n",
    "# Wyświetlenie wyników w kolejności dokładności\n",
    "print(results_df.sort_values(by=\"accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepszy wynik:\n",
    "num_layers              3\n",
    "neurons               128\n",
    "activation           relu\n",
    "optimizer            adam\n",
    "learning_rate        0.01\n",
    "batch_size             64\n",
    "epochs                 50\n",
    "accuracy         0.983333\n",
    "Name: 98, dtype: object\n",
    "     num_layers  neurons activation optimizer  learning_rate  batch_size  \\\n",
    "98            3      128       relu      adam          0.010          64   \n",
    "100           3      128       relu      adam          0.001          32   \n",
    "3             2       64       relu      adam          0.010          64   \n",
    "115           3      128       tanh      adam          0.010          64   \n",
    "101           3      128       relu      adam          0.001          32   \n",
    "..          ...      ...        ...       ...            ...         ...   \n",
    "79            3       64       relu       sgd          0.001          64   \n",
    "110           3      128       relu       sgd          0.001          64   \n",
    "30            2       64       tanh       sgd          0.001          64   \n",
    "14            2       64       relu       sgd          0.001          64   \n",
    "78            3       64       relu       sgd          0.001          64   \n",
    "\n",
    "     epochs  accuracy  \n",
    "98       50  0.983333  \n",
    "100      50  0.980556  \n",
    "3       100  0.980556  \n",
    "115     100  0.977778  \n",
    "101     100  0.977778  \n",
    "..      ...       ...  \n",
    "79      100  0.700000  \n",
    "110      50  0.675000  \n",
    "30       50  0.650000  \n",
    "14       50  0.486111  \n",
    "78       50  0.422222  \n",
    "\n",
    "[128 rows x 8 columns]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
